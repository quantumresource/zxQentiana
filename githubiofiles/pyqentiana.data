import math

from .res_utils import local_linspace, local_logspace, to_rgb
from .cube_to_physical import Qentiana

class PhysicalQubitsVsLogicalError:
    def __init__(self):
        #
        self.nr_items = 100
        # log spaced volume scaling factor
        self.global_v = local_logspace(-2, 2, self.nr_items)
        # scaling factor space
        self.global_s = local_linspace(0.1, 2, self.nr_items)
        #
        self.title = "Phys vs Log Error"
        #
        self.explanation = "The initial circuit is at position (1,1) and any optimization will change the " \
                           "volume and space factor. The final position will show how much resource savings " \
                           "can be expected. Darker colors are better."


    def get_default_parameters(self):
        parameters = {}
        parameters["bool_update_plot"] = False
        parameters["total_num_physical_qubits"] = 500

        return parameters


    def total_err(self, per_unit_err, nr_units):
        # Given a per step error rate
        # calculate/approximate the total error of the computation
        # Equivalent1: The sum of probabilities of at least one (one or more) unit failing with per_unit_err
        # Equivalent2: none of the units fail
        return math.pow(1 - per_unit_err, nr_units)


    def gen_data(self, experiment, parameters = None):
        """
        :param experiment:
        :return:
        """
        nr_log_qubits = experiment["footprint"]
        time = experiment["depth_units"]
        p_err = experiment["physical_error_rate"]

        # parameters are collected by the plot var
        total_num_physical_qubits = parameters["total_num_physical_qubits"]

        data = []

        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                scaled_nr_log_qubits = math.ceil(nr_log_qubits * self.global_s[j])
                scaled_volume = math.ceil(time * self.global_v[i]) * nr_log_qubits

                # this is the distance that fits on patch
                dist = Qentiana.max_distance_to_fit_log_qubits_on_phys_qubits(scaled_nr_log_qubits, total_num_physical_qubits)

                # the per log unit approximated failure is computed from the phys err rate and the distance
                err_per_log_unit = Qentiana.vba_p_logical(p_err, dist)

                # the entire volume will fail with this prob
                err_per_scaled_volume = self.total_err(err_per_log_unit, scaled_volume)

                # // maybe change names for this data array because different meaning of output
                data.append({
                    "x"             : self.global_s[j],
                    "y"             : self.global_v[i],
                    "distance"      : dist,
                    "indiv_error"   : err_per_log_unit,
                    "total_error"   : err_per_scaled_volume,
                    "total_volume"  : scaled_volume,
                    "qubits_used"   : Qentiana.phys_qubits_for_all_log_qubits(scaled_nr_log_qubits, dist)

                })

        return data

    def empty_data(self):
        data = []
        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                # // maybe change names for this data array because different meaning of output
                data.append({
                    "x"             : self.global_s[j],
                    "y"             : self.global_v[i],
                    "distance"      : 0,
                    "indiv_error"   : 0,
                    "total_volume"  : 0,
                    "qubits_used"   : 0,
                    "total_error"   : 0
                })

        return data


    def color_interpretation(self, d):
        rgb = to_rgb(d["total_error"])
        return "rgb({},{},{})".format(rgb, rgb, rgb)


    def explain_data(self, data, experiment):
        return "Distance at point ({}, {}): {} <br>".format(data["x"], data["y"], data["distance"]) \
            + "error rate in unit cell: {} with a total volume of {} <br>".format(data["indiv_error"], data["total_volume"]) \
            + "Total success probability: {} <br>".format(data["total_error"])

import math


# def analysis(data, experiment):
#     """
#
#     :param data:
#     :param experiment:
#     :return:
#     """
#
#     ## The parameters from experiment are scaled using data
#     # the new volume
#     volume_scale = data.y
#     scaled_volume = approx_mult_factor(data.y, experiment.volume)
#
#     qre1 = qre.Qentiana(t_count = 0,
#                         max_logical_qubits = experiment.footprint,
#                         max_time_units = scaled_volume / experiment.footprint,
#                         gate_err_rate = experiment.physical_error_rate)
#
#     # for a volume of size scaled_volume, what is the distance?
#     recalc = qre1.compute_physical_resources()
#
#     # the computed distance is less than the scaling factor
#     is_fine = (recalc["distance"] <= volume_scale)
#
#     ret = {
#         "dist"      : recalc["distance"],
#         "ok"        : is_fine,
#         "threshold" : volume_scale,
#         "volume"    : scaled_volume
#     }
#
#     return ret


# def approx_mult_factor(factor, value):
#     return math.ceil(factor * value)

# TODO: replace with numpy calls
def local_logspace(start, stop, num = 50):
    # Assume base = 10
    ret = [0] * num
    delta = (stop - start) / num

    for i in range(num):
        ret[i] = math.pow(10, start + delta * i)

    return ret


#TODO: replace with numpy calls
def local_linspace(start, stop, num = 50):
    ret = [0] * num
    delta = (stop - start) / num

    for i in range(num):
        ret[i] = start + delta * i

    return ret

#TODO: replace with numpy calls
def local_linspace_2(middle, plus_minus_range, num = 50):
    # assume num is odd
    if num % 2 == 0:
        #make odd
        num += 1

    ret = [0] * num
    middle_index = math.floor(num / 2)
    ret[middle_index] = middle

    float_ratio = (2 * plus_minus_range) / num
    half_distance = middle_index

    for i in range(1, half_distance + 1):
        ret[middle_index + i] = middle + i * float_ratio
        ret[middle_index - i] = middle - i * float_ratio

    return ret

"""
    RGB functions for the interpretation of a value to integers in range 0..255
"""
def to_rgb(param):
    return (255 if (param > 1) else round(param * 255))

def from_rgb(param):
    # why is 2 here?
    return 2 if (param == 255) else (param / 255.0)

import math

from .res_utils import local_logspace, local_linspace, to_rgb
from .cube_to_physical import Qentiana
from .experiment import Experiment

class ResourceSavings:
    def __init__(self):
        #
        self.nr_items = 100
        # log spaced volume scaling factor
        self.global_v = local_logspace(-2, 2, self.nr_items)
        # scaling factor space
        self.global_s = local_linspace(0.1, 2, self.nr_items)
        #
        self.title ="Time AND Space"
        #
        self.explanation = "The initial circuit is at position (1,1) and any optimization will change the " \
                           "volume and space factor. The final position will show how much resource savings " \
                           "can be expected. Darker colors are better."

    def get_default_parameters(self):
        #
        parameters = {}
        parameters["bool_update_plot"] = False

        return parameters


    def gen_data(self, experiment, parameters = None):
        # // 2D Array

        start_time = experiment["depth_units"]
        start_space = experiment["footprint"]
        p_err = experiment["physical_error_rate"]

        data = []

        ex1 = Experiment()
        ex1.props["footprint"] = start_space
        ex1.props["depth_units"] = start_time
        ex1.props["physical_error_rate"] = p_err
        ex1.props["prefer_depth_over_t_count"] = True
        qre1 = Qentiana(ex1.props)
        ret_1 = qre1.compute_physical_resources()

        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                #
                # Two types of scaling are considered simultaneously
                #
                # hardware is scaled
                space_param = math.ceil(self.global_s[j] * start_space)
                # time is scaled -> thus volume is increased
                time_param = math.ceil(self.global_v[i] * start_time)


                # If the initial volume is scaled like this
                ex2 = Experiment()
                ex2.props["footprint"] = space_param
                ex2.props["depth_units"] = time_param
                ex2.props["physical_error_rate"] = p_err
                ex2.props["prefer_depth_over_t_count"] = True
                qre2 = Qentiana(ex2.props)
                ret_2 = qre2.compute_physical_resources()

                ratio = ret_2["num_data_qubits"] / ret_1["num_data_qubits"]

                data.append({
                    "x"                 : self.global_s[j],
                    "y"                 : self.global_v[i],
                    "dist_opt_vol"      : ret_1["distance"],
                    "dist_opt_space"    : ret_2["distance"],
                    "nr_target_vol"     : ret_1["number_of_physical_qubits"],
                    "nr_target_space"   : ret_2["number_of_physical_qubits"],
                    "ratio"             : ratio
                })

        return data



    def empty_data(self):
        data = []
        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                data.append({
                    "x"                 : self.global_s[j],
                    "y"                 : self.global_v[i],
                    "dist_opt_vol"      : 0,
                    "dist_opt_space"    : 0,
                    "nr_target_vol"     : 0,
                    "nr_target_space"   : 0,
                    "ratio"             : 0
                })

        return data

    def color_interpretation(self, d):
        rgb = to_rgb(d["ratio"])
        return "rgb({},{},{})".format(rgb, rgb, rgb)

    def explain_data(self, data, experiment):
        curr_volume = math.ceil(data["y"] * experiment["volume"])
        curr_space = math.ceil(data["x"] * experiment["footprint"])

        return "{} {} -> {} <br>".format(data["x"], data["y"], data["ratio"]) \
               + "dist vol: {} having a footprint of log qubits<br>".format(data["dist_opt_vol"], curr_space) \
               + "dist space: {} for a volume of {}<br>".format(data["dist_opt_space"], curr_volume) \
               + "tradeoff time scaling threshold:{}<br>".format(data["x"] * data["y"]) \
               + "min scaling should be below tradeoff threshold:{}<br>".format(data["dist_opt_space"]) \
               + "qub vol: {}<br>".format(data["nr_target_vol"]) \
               + "qub spc: {}<br>".format(data["nr_target_space"])

import sys
import math

from .res_utils import local_logspace, local_linspace_2
from .cube_to_physical import Qentiana
from .experiment import Experiment

class DistanceBins:
    def __init__(self):
        self.nr_items = 101
        # // log spaced volume scaling factor
        self.x_axis_values = local_linspace_2(1, 0.5, self.nr_items)
        # scaling factor  space
        self.y_axis = local_logspace(2, 8, self.nr_items)
        #
        self.title = "Distance Bins"
        #
        self.explanation = "Tradeoff between volume and total number of " \
                           "physical qubits. Vertical lines are changes in distance."

        self.p___phys_err_rate = 0
        self.p___default_phys_err_rate = 0
        # experiment.physical_error_rate

        # 52 bit integers
        self.p___min_y = sys.maxsize
        self.p___max_y = -sys.maxsize - 1
        #
        # click detector
        self.p___just_clicked = True
        #
        self.reset_min_max_y()

        # A single console message. The same everywhere
        self.message = ""


    def get_default_parameters(self):
        parameters = {}
        parameters["bool_update_plot"] = True

        return parameters

    def reset_min_max_y(self):
        self.p___min_y = sys.maxsize
        self.p___max_y = -sys.maxsize - 1

    def store_min_max_y(self, y_val):
        self.p___min_y = min(y_val, self.p___min_y)
        self.p___max_y = max(y_val, self.p___max_y)

    def gen_data(self, experiment, parameters = None):
        time_orig = experiment["depth_units"]
        space_orig = experiment["footprint"]
        p_err = experiment["physical_error_rate"]

        factor = (100 + experiment["routing_overhead"]) / 100

        data = []

        # stores the volume factors for which the distance changes
        dist_changes = []

        dist_last = -1
        volume_param = 0

        for i in range(len(self.x_axis_values)):
            scaled_time = math.ceil(time_orig * self.x_axis_values[i])

            ex1 = Experiment()
            ex1.props["footprint"] = space_orig
            ex1.props["depth_units"] = scaled_time
            ex1.props["physical_error_rate"] = p_err
            ex1.props["prefer_depth_over_t_count"] = True
            qre = Qentiana(ex1.props)
            ret = qre.compute_physical_resources()

            if (dist_last != -1) and (dist_last != ret["distance"]):
                dist_changes.append({
                    "x": self.x_axis_values[i],
                    "new_dist": ret["distance"]
                    })

            dist_last = ret["distance"]

            to_save_nr_qubits = ret["number_of_physical_qubits"]
            use_data_bus = False

            # Eliminate the ancillas means multiply by 1 / factor
            space_2 = math.ceil(space_orig * (1 / factor))
            # volume_2 = math.ceil(volume_param * (1 / factor))

            ex2 = Experiment()
            ex2.props["footprint"] = space_2
            ex2.props["depth_units"] = scaled_time#volume_2 / space_2
            ex2.props["physical_error_rate"] = p_err
            ex2.props["prefer_depth_over_t_count"] = True
            qre2 = Qentiana(ex2.props)
            ret_vol_2 = qre2.compute_physical_resources()

            #  Increase distance to lower res with data bus
            iterations = 0
            increased_distance = ret_vol_2["distance"]
            qubits_inc_dist = Qentiana.phys_qubits_for_all_log_qubits(space_2, increased_distance)

            while (qubits_inc_dist <= ret["number_of_physical_qubits"]) and (not use_data_bus):
                iterations+=1

                # volume_inc_distance = volume_2 * increased_distance
                # space_orig is not number of patches, but number of logical qubits, and the data bus counts as a qubit

                ex3 = Experiment()
                ex3.props["footprint"] = space_orig
                ex3.props["depth_units"] = scaled_time * increased_distance #volume_inc_distance / space_orig
                ex3.props["physical_error_rate"] = p_err
                ex3.props["prefer_depth_over_t_count"] = True

                qre3 = Qentiana(ex3.props)
                ret_3 = qre3.compute_physical_resources()

                if (ret_3["distance"] <= increased_distance):
                    # this number was calculated for the full layout without data bus
                    to_save_nr_qubits = qubits_inc_dist
                    use_data_bus = True
                else:
                    increased_distance += 2
                    qubits_inc_dist = Qentiana.phys_qubits_for_all_log_qubits(space_2, increased_distance)

            data.append({
                "x": self.x_axis_values[i],
                "number_of_physical_qubits": to_save_nr_qubits,
                "original_number_of_physical_qubits": ret["number_of_physical_qubits"],
                "dist": ret["distance"],
                "use_data_bus": use_data_bus
            })

            if self.x_axis_values[i] == 1.0:
                self.message = "At volume scaling {} with err rate {} <br>".format("1.0", experiment["physical_error_rate"]) \
                        + "there are savings ({}) if footprint overhead is reduced.".format(use_data_bus)

        self.update_y_axis(data)

        return {
            "data": data,
            "dist_changes": dist_changes
        }


    def update_y_axis(self, data):
        """
        Update y axis
        """
        if len(data) == 0:
            return

        self.reset_min_max_y()
        for i in range(len(data)):
            self.store_min_max_y(data[i]["number_of_physical_qubits"])
            self.store_min_max_y(data[i]["original_number_of_physical_qubits"])
        # recompute the values on the axis
        min_log = math.floor(math.log10(self.p___min_y))
        max_log = math.ceil(math.log10(self.p___max_y))
        if (max_log == min_log):
            max_log += 1
        self.y_axis = local_logspace(min_log, max_log, self.nr_items)


    def empty_data(self):
        data = []

        for i in range(len(self.x_axis_values)):
            data.append({
                "x": self.x_axis_values[i],
                "number_of_physical_qubits": 1,
                "original_number_of_physical_qubits": 1,
                "dist": 0,
                "use_data_bus": False
            })

        self.update_y_axis(data)

        ret = {
            "data" : data,
            "dist_changes": []
        }

        return ret

    def explain_data(self, data, experiment):
        # curr_volume = math.ceil(data["y"] * experiment["volume"])
        # curr_space = math.ceil(data["x"] * experiment["footprint"])

        return self.message

"""
From old source
"""
# bus_last_p_logical = -1
# bus_curr_p_logical = -1
# orig_last_p_logical = -1
# orig_curr_p_logical = -1
# # TODO: I think volumes and safety factors should be expressed in datarounds
# bus_last_p_logical = inv_target_error_per_data_round(
# comp_target_error_per_data_round_2(experiment.physical_error_rate, increased_distance),
# volume_inc_distance)
#
# orig_last_p_logical = inv_target_error_per_data_round(
# comp_target_error_per_data_round_2(experiment.physical_error_rate, ret.dist - 2),
# volume_param)

# bus_curr_p_logical = inv_target_error_per_data_round(
# comp_target_error_per_data_round_2(experiment.physical_error_rate, increased_distance),
# volume_inc_distance)
#
# orig_curr_p_logical = inv_target_error_per_data_round(
# comp_target_error_per_data_round_2(experiment.physical_error_rate, ret.dist),
# volume_param);

# increase_percentage = (qubits_inc_dist / ret.number_of_physical_qubits);
# if (this.x_axis_values[i] == 1.0) {
# this.console_text = "At volume scaling " + this.x_axis_values[i] + " with err rate " + experiment.physical_error_rate + "<br>";
#
# this.console_text += "increased by: " + increase_percentage + " compared to original distance " + ret.dist + "<br>";
# this.console_text += "bus_last_p_log: " + bus_last_p_logical + " bus_curr_p_log: " + bus_curr_p_logical + "<br>";
# this.console_text += "orig_last_p_log: " + orig_last_p_logical + " orig_curr_p_log: " + orig_curr_p_logical + "<br>";
#
# this.console_text += "use " + use_data_bus + " it:" + iterations + " from:" + ret_vol_2.dist + " to:" + increased_distance + " from:" + ret.number_of_physical_qubits + " to:" + qubits_inc_dist + "<br>";
#
# // console.log(this.console_text);
# }import math

from .res_utils import local_logspace, local_linspace, to_rgb
from .cube_to_physical import Qentiana
from .experiment import Experiment

class TimeVsSpace:
    def __init__(self):
        #
        self.nr_items = 100
        # log spaced volume scaling factor
        self.global_v = local_logspace(-2, 2, self.nr_items)
        # scaling factor space
        self.global_s = local_linspace(0.1, 2, self.nr_items)
        #
        self.title = "Time OR Space"
        #
        self.explanation = "Comparison of two different optimization heuristics (time and space). " \
                           "In blue/green areas space optimization is better compared to the time optimization. " \
                           "Purple/red areas are not self-consistent (not allowed) and in white areas " \
                           "the time optimization is superior."


    def get_default_parameters(self):
        parameters = {}
        parameters["bool_update_plot"] = False

        return parameters


    def gen_data(self, experiment, parameters = None):
        time_orig = experiment["depth_units"]
        space_orig = experiment["footprint"]
        p_err = experiment["physical_error_rate"]

        data = []

        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                # Scale space
                ex1 = Experiment()
                ex1.props["footprint"] = math.ceil(self.global_s[j] * space_orig)
                ex1.props["depth_units"] = time_orig
                ex1.props["physical_error_rate"] = p_err
                ex1.props["prefer_depth_over_t_count"] = True
                qre1 = Qentiana(ex1.props)
                ret_1 = qre1.compute_physical_resources()

                # Scale time
                ex2 = Experiment()
                ex2.props["footprint"] = space_orig
                ex2.props["depth_units"] = math.ceil(self.global_v[i] * time_orig)
                ex2.props["physical_error_rate"] = p_err
                ex2.props["prefer_depth_over_t_count"] = True
                qre2 = Qentiana(ex2.props)
                ret_2 = qre2.compute_physical_resources()

                ratio = ret_2["num_data_qubits"] / ret_1["num_data_qubits"]

                data.append({
                    "x": self.global_s[j],
                    "y": self.global_v[i],
                    "dist_opt_vol": ret_1["distance"],
                    "dist_opt_space": ret_2["distance"],
                    "nr_target_vol": ret_1["number_of_physical_qubits"],
                    "nr_target_space": ret_2["number_of_physical_qubits"],
                    "ratio": ratio,
                })

        return data


    def empty_data(self):
        data = []
        for i in range(len(self.global_v)):
            for j in range(len(self.global_s)):
                data.append({
                    "x"             : self.global_s[j],
                    "y"             : self.global_v[i],
                    "dist_opt_vol"  : 0,
                    "dist_opt_space": 0,
                    "nr_target_vol" : 0,
                    "nr_target_space": 0,
                    "ratio"         : 0
                })

        return data


    def color_interpretation(self, data):
        component_green     = data["ratio"]
        component_red       = data["ratio"]
        component_blue      = data["ratio"]

        red = to_rgb(component_red)
        green = to_rgb(component_green)
        blue = to_rgb(component_blue)

        # TODO: repair below instead of hardcoded
        # analysis = resu.analysis(data)
        analysis = {"ok": True}

        if analysis["ok"]:
            green = 255
        else:
            red = 255

        return "rgb({}, {}, {})".format(red, green, blue)


    def explain_data(self, data, experiment):
        curr_volume = math.ceil(data["y"] * experiment["volume"])
        curr_space = math.ceil(data["x"] * experiment["footprint"])

        return "{} {} -> {} <br>".format(data["x"], data["y"], data["ratio"]) \
               + "dist vol: {} having a footprint of log qubits<br>".format(data["dist_opt_vol"], curr_space) \
               + "dist space: {} for a volume of {}<br>".format(data["dist_opt_space"], curr_volume) \
               + "tradeoff time scaling threshold:{}<br>".format(data["x"] * data["y"])\
               + "min scaling should be below tradeoff threshold:{}<br>".format(data["dist_opt_space"])\
               + "qub vol: {}<br>".format(data["nr_target_vol"])\
               + "qub spc: {}<br>".format(data["nr_target_space"])


"""
    Add some kind of computation to translate the total volume of a circuit
    to number of physical qubits and number of seconds (time unit)

    ---> Implementation a la Austin
"""

import math

class Qentiana:
    def __init__(self, experiment):
        """
        Constructor
        :param experiment: dictionary of key-values. See Experiment class.
        """
        """
            Parameters
        """
        self.parameters = {
                            # Gate error rate that sets how quickly logical errors are suppressed.
                            # 1e-3 means a factor of 10 suppression with each increase of d by 2.
                            "characteristic_gate_error_rate" : experiment["physical_error_rate"],#0.001,
                            # Time required to execute a single round of the surface code circuit detecting errors.
                            "total_surface_code_cycle_time_ns": 1000,
                            # Safety factor S means that whenever we wish to do N things reliably,
                            # we target a failure probability of 1/(SN).
                            "safety_factor": experiment["safety_factor"],#99,
                            # d1 must be at least 15 to permit enough space for the state injection
                            "l1_distillation_code_distance_d1": 15,
                            # d2 could in principle be less than 15, but it seems unlikely you would
                            # actually want to do that. Max and default is 31.
                            "l2_distillation_code_distance_d2": 31,
                            # Start with 7, but recompute it
                            "data_code_distance": 7,
                            # For the moment assume that the T-depth is not really the worst case in time
                            # Thus, the circuit is Clifford dominated and then its depth is longer
                            # Multiplication factor = 2. used in self.compute_data_code_distance()
                            "multiplication_factor_for_Clifford_domination" : 2,
                            # For the estimation of the depth of a volume (see compute_execution_rounds()) either
                            # option_A: the approximation using T_depth=T_count * multiplication_factor_for_Clifford_domination
                            # option_B: the experiment["depth_units"] is used
                            # option_A is more worst-case like, if for example the depth is only a rule-of-thumb calculation
                            "prefer_depth_over_t_count": experiment["prefer_depth_over_t_count"]}

        #
        #
        # hard coded dimensions of distillation
        # using the dimensions from https://arxiv.org/pdf/1808.06709.pdf
        # These units are distance agnostic
        self.dist_box_dimensions = {"x": 4,
                                    "y": 8,
                                    "t": 6.5,
                                    "distance": -1}


        self.max_logical_qubits = experiment["footprint"]
        self.max_time_units = experiment["depth_units"]
        self.t_count = experiment["t_count"]

        if self.t_count == 0 and self.max_time_units == 0:
            print("PROBLEM!!! Both t_count and max_time_units are zero! Results will be wrong!")

        if self.max_time_units == 0 and self.parameters["prefer_depth_over_t_count"] == True:
            print("PROBLEM!!! The specified depth is zero and t_count is not considered as depth!")


        # A scale is the ratio between data patch qubit distance and the distillation distance
        # It effectively says how many logical qubit patches of distance d
        # can be fitted along the sides of a distillation box
        #
        self.number_of_distillation_levels = self.compute_number_of_dist_levels()
        # lock value
        # self.number_of_distillation_levels = 1
        if self.number_of_distillation_levels not in [1, 2]:
            # I do not know what this is, but should exit
            print("PROBLEM!!! ----> Error! 3+ distillation levels.")


    def compute_dist_box_in_patch_units(self):
        """
        When drawing the surgery diagrams, the distillation boxes have different units compared to the data patches
        The factor is the ratio between the distance of the dist boxes and the data patch distances
        TODO: Correct the computation here. For level2 the distance computation is not correct.
        :return:
        """
        factor = float(self.dist_box_dimensions["depth_distance"]) / float(self.parameters["data_code_distance"])

        n_box_dimensions = {"x" : math.ceil(self.dist_box_dimensions["x"] * factor),
                            "y" : math.ceil(self.dist_box_dimensions["y"] * factor),
                            "t" : math.ceil(self.dist_box_dimensions["t"] * factor)}

        return n_box_dimensions


    def compute_number_of_dist_levels(self):
        #
        #
        char_gate_err = self.parameters["characteristic_gate_error_rate"]
        dist_l1 = self.parameters["l1_distillation_code_distance_d1"]
        dist_l2 = self.parameters["l2_distillation_code_distance_d2"]
        #
        # The same value as the characteristic gate error rate using the techniques of Ying Li.
        injected_d7_error = self.parameters["characteristic_gate_error_rate"]
        # The injected error rate +100 times the distance 7 logical error rate
        # (this approximates the amount of distance 7 structure to do a T gate).
        l1_T_gate_error = injected_d7_error + 100 * self.vba_p_logical(char_gate_err, 7)
        # 1000 times the distance d1 logical error rate approximates the L1 Clifford preparation error.
        l1_Clifford_prep_error = 1000 * self.vba_p_logical(char_gate_err,
                                                           dist_l1)
        # The distilled L1 T gate error rate plus the L1 Clifford preparation error rate.
        l1_output_error = l1_Clifford_prep_error + self.vba_distillation_p_out(l1_T_gate_error, xxx_levels = 1)
        #
        #
        # The L1 output error rate +100 times the distance d1 logical error rate
        # (this approximates the amount of distance d1 structure to do a T gate).
        l2_T_gate_error = l1_output_error + 100 * self.vba_p_logical(char_gate_err,
                                                                     dist_l1)
        # 1000 times the distance d2 logical error rate approximates the L2 Clifford preparation error.
        l2_Clifford_prep_error = 1000 * self.vba_p_logical(char_gate_err,
                                                           dist_l2)
        # The distilled L2 T gate error plus the L2 Clifford preparation error rate.
        l2_output_error = l2_Clifford_prep_error + self.vba_distillation_p_out(l2_T_gate_error, xxx_levels = 1)#aici nu trebuie un doi?

        # Safe target error per T gate.
        local_t_count = self.t_count
        # It could be that the t_count was not specified, so we need a method to approximate it from the specified depth
        # TODO: For the moment, it is the depth in units -- over worst case?
        if local_t_count == 0:
            local_t_count = self.max_time_units

        target_error_per_T_gate = 1 / (self.parameters["safety_factor"] * local_t_count)
        #
        # Required number of distillation levels for this algorithm.
        nr_levels = 2
        if l1_output_error < l2_output_error:
            nr_levels = 1
        elif l2_output_error < target_error_per_T_gate:
            nr_levels = 2
        else:
            nr_levels = "3+, fail"

        return nr_levels


    def compute_distillation_box_distance(self):
        """
            The footprint of L1 and L2 distillations is on the left. The right is the time axis execution.
            ----------------             __________  ____________________  __________
            |A1||A1||A1||A1|            |          ||                    ||          |
            ----------------            |          ||                    ||          |
            ----------------            |     A1   ||                    ||    B1    |
            |              |            |__________||                    ||__________|
            |      2       |                        |        2           |
            |              |             __________ |                    | __________
            ----------------            |          ||                    ||          |      ^
            ----------------            |     A1   ||                    ||    B1    |      |
            |B1||B1||B1||B1|            |          ||                    ||          |      |
            ----------------            |__________||____________________||__________|      | time axis
            After 8 L1 have been executed (on the sides marked with A and B), the next 7 are executed.
            Thus, when L2 is needed, the bounding box in time is determined by max(d2, 2*d1), because either
            a. L2 is not delayed - the L1 sequence can be executed parallel to the L2
            b. L2 is delayed - the L1 sequence takes longer than d2

            Because the L1 and L2 distillations have the same structure,
            the distance acts as a scaling factor of their volume.
        """

        dist_l1 = self.parameters["l1_distillation_code_distance_d1"]
        dist_l2 = self.parameters["l2_distillation_code_distance_d2"]

        """
            The L1 distillations are executed with d1
            The L2 distillations are executed with d2
            Practically, the sides of the footprint are [vertical: 2*8*d1 + 4*d2, horizontal:8*d2]
            We need the bounding box.
            For horizontal: the same discussion like when computing time_dimension
            For vertical: Instead of computing with floats, for the moment round to either 2*(2*8*d1) or 2*(4*d2)
        """

        chosen_distance_in_depth = -1

        if self.number_of_distillation_levels == 1:
            chosen_distance_in_depth = dist_l1

        if (self.number_of_distillation_levels == 2) and (dist_l2 > 2 * dist_l1):
            chosen_distance_in_depth = dist_l2

        if (self.number_of_distillation_levels == 2) and (dist_l2 <= 2 * dist_l1):
            chosen_distance_in_depth = 2 * dist_l1

        self.dist_box_dimensions["depth_distance"] = chosen_distance_in_depth


    def compute_data_code_distance(self):
        """
        Compute data code distance. Consider the maximum number of units on the time axis
        :return:
        """
        """
        Total number of rounds of surface code error detection required to prepare the necessary number of T states.
        T-count is T-depth when all T gates are sequential
        Each distillation has known number of units in time
        The distance has been computed in compute_distillation_scale_factor
        """
        execution_rounds = self.compute_execution_rounds()

        """
        Assume that the logical qubit patches are executed in a sequence
        Each logical qubit is ONE unit long
        And each unit has a distance of execution_rounds (from the distillations)
        """
        total_data_rounds = self.compute_number_of_rounds(elements = self.max_logical_qubits,
                                                          element_distance = execution_rounds,
                                                          element_units_in_time = 1)

        target_error_per_data_round = 1 / (self.parameters["safety_factor"] * total_data_rounds)

        # Code distance required to achieve the safe target error rate per data round.
        self.parameters["data_code_distance"] = self.vba_distance(self.parameters["characteristic_gate_error_rate"],
                                                                  target_error_per_data_round)

    def compute_footprint_distillation_qubits(self):
        """
        Computes the total number of physical qubits necessary for the distillery
        :return:
        """
        footprint_units =  self.dist_box_dimensions["x"] *  self.dist_box_dimensions["y"]
        #
        # Number of qubits associated with a single L1 distillation.
        l1_dist_qubits = footprint_units * self.phys_qubits_per_patch(self.parameters["l1_distillation_code_distance_d1"])
        # Number of qubits associated with a single L2 distillation.
        l2_dist_qubits = footprint_units * self.phys_qubits_per_patch(self.parameters["l2_distillation_code_distance_d2"])
        #
        # Note that if 2 levels of distillation are required, that will be a single L2 + 8 L1 footprints.
        # See diagram above
        total_dist_qubits = "fail"
        if self.number_of_distillation_levels == 1:
            total_dist_qubits = l1_dist_qubits
        elif self.number_of_distillation_levels == 2:
            total_dist_qubits = 8 * l1_dist_qubits + l2_dist_qubits
        else:
            total_dist_qubits = "fail"

        return total_dist_qubits


    def compute_footprint_data_qubits(self):
        # Total number of data qubits, including communication channels.
        # Have unit dimensions -> (1*1)
        # before extraction in method it was the code below
        # self.max_logical_qubits * (1*1) * Qentiana.phys_qubits_per_patch(self.parameters["data_code_distance"])

        num_data_qubits = Qentiana.phys_qubits_for_all_log_qubits(self.max_logical_qubits, self.parameters["data_code_distance"])
        return num_data_qubits


    def compute_physical_resources(self):
        """
            In case max 2 levels distillation
        """
        # update the key "distance" in self.dist_box_dimensions
        self.compute_distillation_box_distance()

        # Compute the data patch distance as an approximation starting from:
        # - T-count / T-depth
        # - number of logical qubits
        # - multiplication_factor_for_Clifford_domination
        self.compute_data_code_distance()

        """
            Compute the number of physical qubits
        """
        num_data_qubits = self.compute_footprint_data_qubits()
        #
        # Total physical qubits required to run algorithm.
        total_qubits = self.compute_footprint_distillation_qubits() + num_data_qubits
        #
        execution_rounds = self.compute_execution_rounds()
        #
        execution_time_secs = execution_rounds * self.parameters["total_surface_code_cycle_time_ns"] * 0.000000001

        results = {
                "levels"                        : self.number_of_distillation_levels,
                "number_of_physical_qubits"     : total_qubits,
                "num_data_qubits"               : num_data_qubits,
                "time"                          : execution_time_secs,
                "distance"                      : self.parameters["data_code_distance"]
        }

        return results


    def compute_execution_rounds(self):
        """
            Compute the number of surface code cycles needed to execute the computation
        """
        execution_rounds = 0

        # if self.max_time_units != 0:
        if self.parameters["prefer_depth_over_t_count"]:
            # Here there is no worst-case, but a computation based on the specified parameter
            execution_rounds = self.compute_number_of_rounds(elements=self.max_time_units,
                                                             element_distance=self.parameters["data_code_distance"],
                                                             element_units_in_time=1)
        else:
            """
            The total execution time is not known, thus T-count is worst-case
            If the T-count is the worst case (and max_time_units was not specified)
            then the multiplication factor is used to determine the max_time_units
            """
            execution_rounds = self.compute_number_of_rounds(elements=self.t_count,
                                          element_distance=self.dist_box_dimensions["depth_distance"],
                                          element_units_in_time=self.dist_box_dimensions["t"])

            execution_rounds *= self.parameters["multiplication_factor_for_Clifford_domination"]

        return execution_rounds

    """
        Counting utilities
    """

    @staticmethod
    def inv_target_error_per_data_round(target_error_data_round, total_elements):
        # Computes a safety factor from target error per data round and total volume
        safety = 1 / (target_error_data_round  * total_elements)
        return safety
        # this is the inverse of
        # target_error_data_round = 1 / (self.parameters["safety_factor"] * total_data_rounds)


    @staticmethod
    def phys_qubits_for_all_log_qubits(nr_logic_qubits, distance):
        # A previous version in Javascript counted also the qubits on the boundaries
        # #first two because there are data and measurement qubits
        # var tmp = space * (2 * distance * distance - 1);
        # #add qubits on two boundaries(e.g.bottom, right)
        # return tmp + 2 * space * (distance * 2 + 1);
        return nr_logic_qubits * Qentiana.phys_qubits_per_patch(distance)


    @staticmethod
    def phys_qubits_per_patch(distance):
        """
        How many data and syndrome qubits are necessary per patch for a given distance?
        :param distance:
        :return:
        """
        return 2 * (distance ** 2)

    @staticmethod
    def distance_from_patch_phys_qubits(phys_qubits):
        """
        The inverse operation of phys_qubits_per_patch. Assume square patch.
        :param phys_qubits: total data and syndrome qubits for the patch
        :return: estimated distance available on a square patch
        """
        return math.floor(math.sqrt(phys_qubits/2))

    @staticmethod
    def max_distance_to_fit_log_qubits_on_phys_qubits(logical_qubits, total_physical_qubits):
        # Calculates the maximum distance given a fixed number of physical qubits
        # Assuming that a square number of physical qubits is necessary to encode a logical qubit

        if logical_qubits > total_physical_qubits:
            # Error: there are more logical qubits than physical
            return -1

        max_phys_qubits_per_logical = total_physical_qubits / logical_qubits

        ret = Qentiana.distance_from_patch_phys_qubits(max_phys_qubits_per_logical)

        if ret <= 2:
            return 1

        return ret

    @staticmethod
    def compute_number_of_rounds(elements, element_distance, element_units_in_time):
        """
        Computes for a number of sequentially executed elements, the number of error correction rounds.
        :param elements: number of elements (e.g. distillation procedures)
        :param element_distance: the distance of the surface code used for the considered element types
        :param element_units_in_time: number of units (agnostic of distance) in time
        :return:
        """
        return elements * element_distance * element_units_in_time

    """
    VBA from Austin's arxiv Excel
    Do While translated to simple while, because condition was after DO
    """

    @staticmethod
    def vba_levels(p_in, p_out):
        n = 0
        while p_in > p_out:
            n = n + 1
            p_in = 35 * (p_in ** 3)
        # levels = n
        return n

    @staticmethod
    def vba_p_logical(p_gate, d):
        return 0.1 * (100 * p_gate) ** ((d + 1) / 2)

    @staticmethod
    def vba_distance(p_gate, p_l):
        d = 3
        while Qentiana.vba_p_logical(p_gate, d) > p_l:
            d = d + 2
        # distance = d
        return d

    @staticmethod
    def vba_distillation_p_out(p_in, xxx_levels):
        n = 0
        while n < xxx_levels:
            n = n + 1
            p_in = 35 * (p_in ** 3)
        # distillation_p_out = p_in
        return p_in

import json


class Experiment:
    def __init__(self):
        self.name = ""

        self.props = {}

        """
        The number of T gates in the circuit
        """
        self.props["t_count"] = 0

        """
        The footprint is the total number
            of data qubits which includes
            - computational qubits
            - ancilla needed to intermediate between the computational qubits
        """
        self.props["footprint"] = 0

        # the total number of gates in the circuit
        # max_time_units
        self.props["depth_units"] = 0

        """
            For the purpose of lattice surgery layouts, the routing overhead is most of the times 50%
            This means that: 
                footprint = data_qubits + routing_overhead * data_qubits
                footprint = (1 + routing_overhead) * data_qubits
        """
        self.props["routing_overhead"] = 50

        # gate_err_rate
        self.props["physical_error_rate"] = 0.001

        self.props["bool_distance"] = False
        self.props["enforced_distance"]  = 0

        self.props["safety_factor"] = 99

        # For the estimation of the depth of a volume (see compute_execution_rounds()) either
        # option_A: the approximation using T_depth=T_count * multiplication_factor_for_Clifford_domination
        # option_B: the experiment["depth_units"] is used
        # option_A is more worst-case like, if for example the depth is only a rule-of-thumb calculation
        self.props["prefer_depth_over_t_count"] = False


    def to_json(self):
        obj = {}
        obj[self.name] = self.props
        return json.dumps(obj)


